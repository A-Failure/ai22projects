{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cv_imread(filepath):\n",
    "    img = cv2.imdecode(np.fromfile(filepath, dtype=np.uint8), -1)\n",
    "    return img\n",
    " \n",
    " \n",
    "# 图像处理\n",
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4, 0.4, 0.4], std=[0.2, 0.2, 0.2])\n",
    "])\n",
    "dataset = ImageFolder(r'.\\archive\\raw-img', transform=transform)\n",
    " \n",
    "train = int(len(dataset) * 0.8)\n",
    "other_train = len(dataset) - train\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train, other_train])\n",
    " \n",
    "train_dataloader_train = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "train_dataloader_test = DataLoader(test_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\22973\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Net, self).__init__()\n",
    "        for name, value in model.named_parameters():\n",
    "            value.requires_grad = False\n",
    "            \n",
    "        self.vgg_layer = nn.Sequential(*list(model.children())[:-2])\n",
    "        # 第一层\n",
    "        self.Linear_layer1 = nn.Linear(512, 4096)\n",
    "        # 第二层\n",
    "        self.Linear_layer2 = nn.Linear(4096, 512)\n",
    "        # 第三层\n",
    "        self.Linear_layer3 = nn.Linear(512, 10)\n",
    "        # drop层\n",
    "        self.drop_layer = torch.nn.Dropout(p=0.5)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.vgg_layer(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.Linear_layer1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.Linear_layer2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.Linear_layer3(x)\n",
    "        return x\n",
    " \n",
    " \n",
    "vgg = models.vgg16(pretrained=True)\n",
    "model = Net(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "max_epoch = 5\n",
    "batch_size = 1024\n",
    "max_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 || batch:1/21 average_loss: 2.325 || train_acc: 0.12 || loss: 2.33\n",
      "Epoch: 1/5 || batch:2/21 average_loss: 4.149 || train_acc: 0.21 || loss: 4.15\n",
      "Epoch: 1/5 || batch:3/21 average_loss: 2.444 || train_acc: 0.20 || loss: 2.44\n",
      "Epoch: 1/5 || batch:4/21 average_loss: 1.999 || train_acc: 0.32 || loss: 2.00\n",
      "Epoch: 1/5 || batch:5/21 average_loss: 1.931 || train_acc: 0.34 || loss: 1.93\n",
      "Epoch: 1/5 || batch:6/21 average_loss: 1.814 || train_acc: 0.43 || loss: 1.81\n",
      "Epoch: 1/5 || batch:7/21 average_loss: 1.736 || train_acc: 0.47 || loss: 1.74\n",
      "Epoch: 1/5 || batch:8/21 average_loss: 1.673 || train_acc: 0.48 || loss: 1.67\n",
      "Epoch: 1/5 || batch:9/21 average_loss: 1.544 || train_acc: 0.50 || loss: 1.54\n",
      "Epoch: 1/5 || batch:10/21 average_loss: 1.468 || train_acc: 0.54 || loss: 1.47\n",
      "Epoch: 1/5 || batch:11/21 average_loss: 1.423 || train_acc: 0.53 || loss: 1.42\n",
      "Epoch: 1/5 || batch:12/21 average_loss: 1.313 || train_acc: 0.54 || loss: 1.31\n",
      "Epoch: 1/5 || batch:13/21 average_loss: 1.272 || train_acc: 0.54 || loss: 1.27\n",
      "Epoch: 1/5 || batch:14/21 average_loss: 1.252 || train_acc: 0.56 || loss: 1.25\n",
      "Epoch: 1/5 || batch:15/21 average_loss: 1.265 || train_acc: 0.56 || loss: 1.27\n",
      "Epoch: 1/5 || batch:16/21 average_loss: 1.235 || train_acc: 0.57 || loss: 1.24\n",
      "Epoch: 1/5 || batch:17/21 average_loss: 1.214 || train_acc: 0.57 || loss: 1.21\n",
      "Epoch: 1/5 || batch:18/21 average_loss: 1.227 || train_acc: 0.59 || loss: 1.23\n",
      "Epoch: 1/5 || batch:19/21 average_loss: 1.215 || train_acc: 0.58 || loss: 1.21\n",
      "Epoch: 1/5 || batch:20/21 average_loss: 1.164 || train_acc: 0.59 || loss: 1.16\n",
      "Epoch: 1/5 || batch:21/21 average_loss: 1.102 || train_acc: 0.62 || loss: 1.10\n",
      "Epoch: 1/5 || acc: 0 || all_loss: 34.76\n",
      "Epoch: 2/5 || batch:1/21 average_loss: 1.135 || train_acc: 0.60 || loss: 1.13\n",
      "Epoch: 2/5 || batch:2/21 average_loss: 1.112 || train_acc: 0.63 || loss: 1.11\n",
      "Epoch: 2/5 || batch:3/21 average_loss: 1.049 || train_acc: 0.64 || loss: 1.05\n",
      "Epoch: 2/5 || batch:4/21 average_loss: 1.078 || train_acc: 0.62 || loss: 1.08\n",
      "Epoch: 2/5 || batch:5/21 average_loss: 1.051 || train_acc: 0.64 || loss: 1.05\n",
      "Epoch: 2/5 || batch:6/21 average_loss: 1.021 || train_acc: 0.63 || loss: 1.02\n",
      "Epoch: 2/5 || batch:7/21 average_loss: 1.055 || train_acc: 0.65 || loss: 1.05\n",
      "Epoch: 2/5 || batch:8/21 average_loss: 0.977 || train_acc: 0.65 || loss: 0.98\n",
      "Epoch: 2/5 || batch:9/21 average_loss: 1.057 || train_acc: 0.63 || loss: 1.06\n",
      "Epoch: 2/5 || batch:10/21 average_loss: 1.016 || train_acc: 0.67 || loss: 1.02\n",
      "Epoch: 2/5 || batch:11/21 average_loss: 0.955 || train_acc: 0.69 || loss: 0.96\n",
      "Epoch: 2/5 || batch:12/21 average_loss: 0.971 || train_acc: 0.66 || loss: 0.97\n",
      "Epoch: 2/5 || batch:13/21 average_loss: 0.919 || train_acc: 0.67 || loss: 0.92\n",
      "Epoch: 2/5 || batch:14/21 average_loss: 0.957 || train_acc: 0.68 || loss: 0.96\n",
      "Epoch: 2/5 || batch:15/21 average_loss: 0.938 || train_acc: 0.67 || loss: 0.94\n",
      "Epoch: 2/5 || batch:16/21 average_loss: 0.974 || train_acc: 0.68 || loss: 0.97\n",
      "Epoch: 2/5 || batch:17/21 average_loss: 0.979 || train_acc: 0.66 || loss: 0.98\n",
      "Epoch: 2/5 || batch:18/21 average_loss: 0.954 || train_acc: 0.67 || loss: 0.95\n",
      "Epoch: 2/5 || batch:19/21 average_loss: 0.992 || train_acc: 0.67 || loss: 0.99\n",
      "Epoch: 2/5 || batch:20/21 average_loss: 0.976 || train_acc: 0.67 || loss: 0.98\n",
      "Epoch: 2/5 || batch:21/21 average_loss: 0.921 || train_acc: 0.67 || loss: 0.92\n",
      "Epoch: 2/5 || acc: 0 || all_loss: 21.09\n",
      "Epoch: 3/5 || batch:1/21 average_loss: 0.972 || train_acc: 0.67 || loss: 0.97\n",
      "Epoch: 3/5 || batch:2/21 average_loss: 0.837 || train_acc: 0.71 || loss: 0.84\n",
      "Epoch: 3/5 || batch:3/21 average_loss: 0.949 || train_acc: 0.65 || loss: 0.95\n",
      "Epoch: 3/5 || batch:4/21 average_loss: 0.817 || train_acc: 0.73 || loss: 0.82\n",
      "Epoch: 3/5 || batch:5/21 average_loss: 0.871 || train_acc: 0.69 || loss: 0.87\n",
      "Epoch: 3/5 || batch:6/21 average_loss: 0.961 || train_acc: 0.67 || loss: 0.96\n",
      "Epoch: 3/5 || batch:7/21 average_loss: 0.876 || train_acc: 0.71 || loss: 0.88\n",
      "Epoch: 3/5 || batch:8/21 average_loss: 0.894 || train_acc: 0.69 || loss: 0.89\n",
      "Epoch: 3/5 || batch:9/21 average_loss: 0.816 || train_acc: 0.70 || loss: 0.82\n",
      "Epoch: 3/5 || batch:10/21 average_loss: 0.868 || train_acc: 0.71 || loss: 0.87\n",
      "Epoch: 3/5 || batch:11/21 average_loss: 0.827 || train_acc: 0.71 || loss: 0.83\n",
      "Epoch: 3/5 || batch:12/21 average_loss: 0.832 || train_acc: 0.71 || loss: 0.83\n",
      "Epoch: 3/5 || batch:13/21 average_loss: 0.864 || train_acc: 0.72 || loss: 0.86\n",
      "Epoch: 3/5 || batch:14/21 average_loss: 0.871 || train_acc: 0.69 || loss: 0.87\n",
      "Epoch: 3/5 || batch:15/21 average_loss: 0.909 || train_acc: 0.69 || loss: 0.91\n",
      "Epoch: 3/5 || batch:16/21 average_loss: 0.833 || train_acc: 0.73 || loss: 0.83\n",
      "Epoch: 3/5 || batch:17/21 average_loss: 0.818 || train_acc: 0.72 || loss: 0.82\n",
      "Epoch: 3/5 || batch:18/21 average_loss: 0.873 || train_acc: 0.70 || loss: 0.87\n",
      "Epoch: 3/5 || batch:19/21 average_loss: 0.846 || train_acc: 0.71 || loss: 0.85\n",
      "Epoch: 3/5 || batch:20/21 average_loss: 0.865 || train_acc: 0.70 || loss: 0.87\n",
      "Epoch: 3/5 || batch:21/21 average_loss: 0.837 || train_acc: 0.70 || loss: 0.84\n",
      "Epoch: 3/5 || acc: 0 || all_loss: 18.24\n",
      "Epoch: 4/5 || batch:1/21 average_loss: 0.769 || train_acc: 0.74 || loss: 0.77\n",
      "Epoch: 4/5 || batch:2/21 average_loss: 0.780 || train_acc: 0.74 || loss: 0.78\n",
      "Epoch: 4/5 || batch:3/21 average_loss: 0.784 || train_acc: 0.72 || loss: 0.78\n",
      "Epoch: 4/5 || batch:4/21 average_loss: 0.782 || train_acc: 0.73 || loss: 0.78\n",
      "Epoch: 4/5 || batch:5/21 average_loss: 0.780 || train_acc: 0.72 || loss: 0.78\n",
      "Epoch: 4/5 || batch:6/21 average_loss: 0.828 || train_acc: 0.72 || loss: 0.83\n",
      "Epoch: 4/5 || batch:7/21 average_loss: 0.733 || train_acc: 0.75 || loss: 0.73\n",
      "Epoch: 4/5 || batch:8/21 average_loss: 0.828 || train_acc: 0.71 || loss: 0.83\n",
      "Epoch: 4/5 || batch:9/21 average_loss: 0.761 || train_acc: 0.75 || loss: 0.76\n",
      "Epoch: 4/5 || batch:10/21 average_loss: 0.849 || train_acc: 0.73 || loss: 0.85\n",
      "Epoch: 4/5 || batch:11/21 average_loss: 0.796 || train_acc: 0.72 || loss: 0.80\n",
      "Epoch: 4/5 || batch:12/21 average_loss: 0.763 || train_acc: 0.75 || loss: 0.76\n",
      "Epoch: 4/5 || batch:13/21 average_loss: 0.794 || train_acc: 0.72 || loss: 0.79\n",
      "Epoch: 4/5 || batch:14/21 average_loss: 0.810 || train_acc: 0.73 || loss: 0.81\n",
      "Epoch: 4/5 || batch:15/21 average_loss: 0.792 || train_acc: 0.74 || loss: 0.79\n",
      "Epoch: 4/5 || batch:16/21 average_loss: 0.819 || train_acc: 0.71 || loss: 0.82\n",
      "Epoch: 4/5 || batch:17/21 average_loss: 0.751 || train_acc: 0.74 || loss: 0.75\n",
      "Epoch: 4/5 || batch:18/21 average_loss: 0.791 || train_acc: 0.74 || loss: 0.79\n",
      "Epoch: 4/5 || batch:19/21 average_loss: 0.816 || train_acc: 0.72 || loss: 0.82\n",
      "Epoch: 4/5 || batch:20/21 average_loss: 0.797 || train_acc: 0.72 || loss: 0.80\n",
      "Epoch: 4/5 || batch:21/21 average_loss: 0.799 || train_acc: 0.73 || loss: 0.80\n",
      "Epoch: 4/5 || acc: 0 || all_loss: 16.62\n",
      "Epoch: 5/5 || batch:1/21 average_loss: 0.749 || train_acc: 0.75 || loss: 0.75\n",
      "Epoch: 5/5 || batch:2/21 average_loss: 0.667 || train_acc: 0.78 || loss: 0.67\n",
      "Epoch: 5/5 || batch:3/21 average_loss: 0.755 || train_acc: 0.75 || loss: 0.75\n",
      "Epoch: 5/5 || batch:4/21 average_loss: 0.723 || train_acc: 0.76 || loss: 0.72\n",
      "Epoch: 5/5 || batch:5/21 average_loss: 0.726 || train_acc: 0.76 || loss: 0.73\n",
      "Epoch: 5/5 || batch:6/21 average_loss: 0.724 || train_acc: 0.77 || loss: 0.72\n",
      "Epoch: 5/5 || batch:7/21 average_loss: 0.755 || train_acc: 0.75 || loss: 0.75\n",
      "Epoch: 5/5 || batch:8/21 average_loss: 0.703 || train_acc: 0.75 || loss: 0.70\n",
      "Epoch: 5/5 || batch:9/21 average_loss: 0.703 || train_acc: 0.76 || loss: 0.70\n",
      "Epoch: 5/5 || batch:10/21 average_loss: 0.732 || train_acc: 0.74 || loss: 0.73\n",
      "Epoch: 5/5 || batch:11/21 average_loss: 0.696 || train_acc: 0.77 || loss: 0.70\n",
      "Epoch: 5/5 || batch:12/21 average_loss: 0.722 || train_acc: 0.75 || loss: 0.72\n",
      "Epoch: 5/5 || batch:13/21 average_loss: 0.743 || train_acc: 0.74 || loss: 0.74\n",
      "Epoch: 5/5 || batch:14/21 average_loss: 0.712 || train_acc: 0.76 || loss: 0.71\n",
      "Epoch: 5/5 || batch:15/21 average_loss: 0.737 || train_acc: 0.74 || loss: 0.74\n",
      "Epoch: 5/5 || batch:16/21 average_loss: 0.742 || train_acc: 0.73 || loss: 0.74\n",
      "Epoch: 5/5 || batch:17/21 average_loss: 0.696 || train_acc: 0.76 || loss: 0.70\n",
      "Epoch: 5/5 || batch:18/21 average_loss: 0.765 || train_acc: 0.72 || loss: 0.76\n",
      "Epoch: 5/5 || batch:19/21 average_loss: 0.729 || train_acc: 0.76 || loss: 0.73\n",
      "Epoch: 5/5 || batch:20/21 average_loss: 0.711 || train_acc: 0.74 || loss: 0.71\n",
      "Epoch: 5/5 || batch:21/21 average_loss: 0.764 || train_acc: 0.72 || loss: 0.76\n",
      "Epoch: 5/5 || acc: 0 || all_loss: 15.25\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    " \n",
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    batch = 0\n",
    "    all_loss = 0\n",
    "    trail_acc = 0\n",
    "    trail_totle = 0\n",
    "    allax_batch = len(train_dataloader_train)\n",
    "    for train_data in train_dataloader_train:\n",
    "        batch_images, batch_labels = train_data\n",
    "        out = model(batch_images)\n",
    "        # loss\n",
    "        loss = loss_func(out, batch_labels)\n",
    "        all_loss += loss\n",
    "        # 预测\n",
    "        prediction = torch.max(out, 1)[1]\n",
    "        # 总预测准确的数量\n",
    "        train_correct = (prediction == batch_labels).sum()\n",
    "        # 加和数量\n",
    "        trail_acc += train_correct\n",
    "        # 总数量\n",
    "        trail_totle += len(batch_labels)\n",
    "        # 求导\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传递\n",
    "        loss.backward()\n",
    "        # 向前走一步\n",
    "        optimizer.step()\n",
    "        batch += 1\n",
    "        print(\"Epoch: %d/%d || batch:%d/%d average_loss: %.3f || train_acc: %.2f || loss: %.2f\"\n",
    "              % (epoch + 1, max_epoch, batch, allax_batch, loss, train_correct / len(batch_labels), loss))\n",
    "    print(\"Epoch: %d/%d || acc: %d || all_loss: %.2f\" % (epoch + 1, max_epoch, trail_acc / trail_totle, all_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3882\n",
      "           1       0.68      0.74      0.71      2133\n",
      "           2       0.77      0.76      0.77      1147\n",
      "           3       0.81      0.85      0.83      1676\n",
      "           4       0.81      0.84      0.82      2463\n",
      "           5       0.74      0.58      0.65      1335\n",
      "           6       0.74      0.56      0.63      1524\n",
      "           7       0.71      0.66      0.68      1429\n",
      "           8       0.87      0.91      0.89      3859\n",
      "           9       0.76      0.66      0.70      1495\n",
      "\n",
      "    accuracy                           0.77     20943\n",
      "   macro avg       0.76      0.73      0.74     20943\n",
      "weighted avg       0.77      0.77      0.77     20943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4, 0.4, 0.4], std=[0.2, 0.2, 0.2])\n",
    "])\n",
    " \n",
    "with torch.no_grad():\n",
    "    true_lable = []\n",
    "    pre_lable = []\n",
    "    for train_data in train_dataloader_train:\n",
    "        batch_images, batch_labels = train_data\n",
    "        out = model(batch_images)\n",
    "        prediction = torch.max(out, 1)[1]\n",
    "        true_lable += batch_labels.tolist()\n",
    "        pre_lable += prediction.tolist()\n",
    " \n",
    "print(classification_report(true_lable, pre_lable))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "345dc496badca7fcd086a74cfba6bc12e9fdf6ba88b37445ccf0d2e1c1d9627b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
